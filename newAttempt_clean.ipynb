{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-19 21:46:59.108269: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-10-19 21:46:59.108289: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import layers\n",
    "import os \n",
    "import PIL\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diabetic_retinopathy_detection/btgraham-300\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'diabetic_retinopathy_detection/btgraham-300'\n",
    "subdataset_name = 'btgraham-300'\n",
    "print(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfds.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-19 21:47:05.794051: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-10-19 21:47:05.794081: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-10-19 21:47:05.794104: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (juanpablo): /proc/driver/nvidia/version does not exist\n",
      "2021-10-19 21:47:05.794428: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:absl:options.experimental_threading is deprecated. Use options.threading instead.\n",
      "WARNING:absl:options.experimental_threading is deprecated. Use options.threading instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset, info= tfds.load(dataset_name,split=tfds.Split.TRAIN, download=True, with_info=True)\n",
    "#dataset = dataset.map(lambda image: tf.image.resize_with_crop_or_pad(image, 64, 64))\n",
    "print(type(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(35126, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 300\n",
    "\n",
    "resize_and_rescale = tf.keras.Sequential([\n",
    "  layers.Resizing(IMG_SIZE, IMG_SIZE),\n",
    "  layers.Rescaling(1./255)\n",
    "])\n",
    "\n",
    "# Ejemplo de uso\n",
    "# result = resize_and_rescale(image['image'])\n",
    "# _ = plt.imshow(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = BATCH_SIZE\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def prepare(ds):\n",
    "  # Resize and rescale all datasets.\n",
    "  ds = ds.map(lambda x: (resize_and_rescale(x['image'])), \n",
    "              num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "  #if shuffle:\n",
    "  #  ds = ds.shuffle(1000)\n",
    "\n",
    "  # Batch all datasets.\n",
    "  #ds = ds.batch(batch_size)\n",
    "\n",
    "  # Use data augmentation only on the training set.\n",
    "  #if augment:\n",
    "  #  ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y), \n",
    "  #              num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "  # Use buffered prefetching on all datasets.\n",
    "  return ds.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "  aKernelSize = 5\n",
    "  aStrides = 2\n",
    "  z_dim = 100\n",
    "\n",
    "  layersInfo = [(4, 1024), (8, 512), (16,256), (32, 128), (64, 3)]\n",
    "  \n",
    "  layer_0 = (4, 1024)\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(layers.Dense(layer_0[0] ** 2 * layer_0[1], use_bias=False, input_shape=(z_dim,), name='gen_1'))\n",
    "  model.add(layers.BatchNormalization())\n",
    "  model.add(layers.LeakyReLU())\n",
    "\n",
    "  \n",
    "  model.add(layers.Reshape((layer_0[0], layer_0[0], layer_0[1])))\n",
    "  print(model.output_shape)\n",
    "  assert model.output_shape == (None, layer_0[0], layer_0[0], layer_0[1])  # Note: None is the batch size\n",
    "\n",
    "  layer_1 = layersInfo[1]\n",
    "  model.add(layers.Conv2DTranspose(layer_1[1], kernel_size=aKernelSize, strides=aStrides, padding='same', use_bias=False, name='gen_2'))\n",
    "  print(model.output_shape)\n",
    "  assert model.output_shape == (None, layer_1[0], layer_1[0], layer_1[1])\n",
    "  model.add(layers.BatchNormalization())\n",
    "  model.add(layers.LeakyReLU())\n",
    "\n",
    "  layer_2 = layersInfo[2]\n",
    "  model.add(layers.Conv2DTranspose(layer_2[1], kernel_size=aKernelSize, strides=aStrides, padding='same', use_bias=False, name='gen_3'))\n",
    "  assert model.output_shape == (None, layer_2[0], layer_2[0], layer_2[1])\n",
    "  model.add(layers.BatchNormalization())\n",
    "  model.add(layers.LeakyReLU())\n",
    "\n",
    "  layer_3 = layersInfo[3]\n",
    "  model.add(layers.Conv2DTranspose(layer_3[1], kernel_size=aKernelSize, strides=aStrides, padding='same', use_bias=False, name='gen_4'))\n",
    "  assert model.output_shape == (None, layer_3[0], layer_3[0], layer_3[1])\n",
    "  model.add(layers.BatchNormalization())\n",
    "  model.add(layers.LeakyReLU())\n",
    "\n",
    "  layer_4 = layersInfo[4]\n",
    "  model.add(layers.Conv2DTranspose(layer_4[1], kernel_size=aKernelSize, strides=aStrides, padding='same', use_bias=False, activation='tanh', name='gen_5'))\n",
    "  assert model.output_shape == (None, layer_4[0], layer_4[0], layer_4[1])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 4, 4, 1024)\n",
      "(None, 8, 8, 512)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6a654b2e20>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = make_generator_model()\n",
    "\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "  aKernelSize = 5\n",
    "  aStrides = 2\n",
    "  dropOutRatio = 0.3\n",
    "\n",
    "  layersInfo = [(32, 64), (16,128), (8, 256), (4, 512)]\n",
    "  \n",
    "  model = tf.keras.Sequential()\n",
    "  \n",
    "  layer_0 = layersInfo[0]\n",
    "  model.add(layers.Conv2D(layer_0[1], kernel_size=aKernelSize, strides=aStrides, padding='same', input_shape=[64, 64, 3], name='dis_1'))\n",
    "  model.add(layers.LeakyReLU())\n",
    "  model.add(layers.Dropout(dropOutRatio))\n",
    "\n",
    "  layer_1 = layersInfo[1]\n",
    "  model.add(layers.Conv2D(layer_1[1], kernel_size=aKernelSize, strides=aStrides, padding='same', name='dis_2'))\n",
    "  model.add(layers.LeakyReLU())\n",
    "  model.add(layers.Dropout(dropOutRatio))\n",
    "\n",
    "  layer_2 = layersInfo[2]\n",
    "  model.add(layers.Conv2D(layer_2[1], kernel_size=aKernelSize, strides=aStrides, padding='same', name='dis_3'))\n",
    "  model.add(layers.LeakyReLU())\n",
    "  model.add(layers.Dropout(dropOutRatio))\n",
    "\n",
    "  layer_3 = layersInfo[3]\n",
    "  model.add(layers.Conv2D(layer_3[1], kernel_size=aKernelSize, strides=aStrides, padding='same', name='dis_4'))\n",
    "  model.add(layers.LeakyReLU())\n",
    "  model.add(layers.Dropout(dropOutRatio))\n",
    "\n",
    "  model.add(layers.Flatten())\n",
    "  model.add(layers.Dense(1))\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[7.429183e-05]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image)\n",
    "print (decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<keras.layers.convolutional.Conv2D object at 0x7f6a654deb50>, <keras.layers.advanced_activations.LeakyReLU object at 0x7f6a65a69a00>, <keras.layers.core.Dropout object at 0x7f6a65a413a0>, <keras.layers.convolutional.Conv2D object at 0x7f6a654721c0>, <keras.layers.advanced_activations.LeakyReLU object at 0x7f6a65472310>, <keras.layers.core.Dropout object at 0x7f6a9f023e80>, <keras.layers.convolutional.Conv2D object at 0x7f6a653f7dc0>, <keras.layers.advanced_activations.LeakyReLU object at 0x7f6a653f7040>, <keras.layers.core.Dropout object at 0x7f6a653fc790>, <keras.layers.convolutional.Conv2D object at 0x7f6a65405ee0>, <keras.layers.advanced_activations.LeakyReLU object at 0x7f6a6540cbe0>, <keras.layers.core.Dropout object at 0x7f6a65405880>, <keras.layers.core.Flatten object at 0x7f6a653fcf40>, <keras.layers.core.Dense object at 0x7f6a65414520>]\n"
     ]
    }
   ],
   "source": [
    "print(discriminator.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learing_rate = 1e-4\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learing_rate)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learing_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# You will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_images = generator(noise, training=True)\n",
    "\n",
    "      real_output = discriminator(images, training=True)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(epoch_no):\n",
    "  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "  predictions = model(test_input, training=False)\n",
    "\n",
    "  fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "      plt.axis('off')\n",
    "\n",
    "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for image_batch in dataset:\n",
    "      train_step(image_batch)\n",
    "\n",
    "    # Save the model every 15 epochs\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "  # Generate after the final epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 64, 64, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 64, 64, 3), dtype=tf.float32, name='dis_1_input'), name='dis_1_input', description=\"created by layer 'dis_1_input'\"), but it was called on an input with incompatible shape (256, 300, 300, 3).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 64, 64, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 64, 64, 3), dtype=tf.float32, name='dis_1_input'), name='dis_1_input', description=\"created by layer 'dis_1_input'\"), but it was called on an input with incompatible shape (256, 300, 300, 3).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /tmp/ipykernel_156546/3248131820.py:10 train_step  *\n        real_output = discriminator(images, training=True)\n    /home/juan/.local/lib/python3.8/site-packages/keras/engine/base_layer.py:1037 __call__  **\n        outputs = call_fn(inputs, *args, **kwargs)\n    /home/juan/.local/lib/python3.8/site-packages/keras/engine/sequential.py:369 call\n        return super(Sequential, self).call(inputs, training=training, mask=mask)\n    /home/juan/.local/lib/python3.8/site-packages/keras/engine/functional.py:414 call\n        return self._run_internal_graph(\n    /home/juan/.local/lib/python3.8/site-packages/keras/engine/functional.py:550 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    /home/juan/.local/lib/python3.8/site-packages/keras/engine/base_layer.py:1020 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /home/juan/.local/lib/python3.8/site-packages/keras/engine/input_spec.py:250 assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer dense is incompatible with the layer: expected axis -1 of input shape to have value 8192 but received input with shape (256, 184832)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_156546/899478657.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_156546/2044935595.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimage_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m       \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Save the model every 15 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3036\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m-> 3038\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3296\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3297\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3298\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3299\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3300\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /tmp/ipykernel_156546/3248131820.py:10 train_step  *\n        real_output = discriminator(images, training=True)\n    /home/juan/.local/lib/python3.8/site-packages/keras/engine/base_layer.py:1037 __call__  **\n        outputs = call_fn(inputs, *args, **kwargs)\n    /home/juan/.local/lib/python3.8/site-packages/keras/engine/sequential.py:369 call\n        return super(Sequential, self).call(inputs, training=training, mask=mask)\n    /home/juan/.local/lib/python3.8/site-packages/keras/engine/functional.py:414 call\n        return self._run_internal_graph(\n    /home/juan/.local/lib/python3.8/site-packages/keras/engine/functional.py:550 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    /home/juan/.local/lib/python3.8/site-packages/keras/engine/base_layer.py:1020 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /home/juan/.local/lib/python3.8/site-packages/keras/engine/input_spec.py:250 assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer dense is incompatible with the layer: expected axis -1 of input shape to have value 8192 but received input with shape (256, 184832)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = prepare(dataset).batch(BATCH_SIZE)\n",
    "train(train_dataset, EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
