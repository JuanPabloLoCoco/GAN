{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import layers\n",
    "import os \n",
    "import PIL\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diabetic_retinopathy_detection/btgraham-300\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'diabetic_retinopathy_detection/btgraham-300'\n",
    "subdataset_name = 'btgraham-300'\n",
    "print(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.0'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfds.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:options.experimental_threading is deprecated. Use options.threading instead.\n",
      "WARNING:absl:options.experimental_threading is deprecated. Use options.threading instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset, info= tfds.load(dataset_name,split=tfds.Split.TRAIN, download=True, with_info=True)\n",
    "#dataset = dataset.map(lambda image: tf.image.resize_with_crop_or_pad(image, 64, 64))\n",
    "print(type(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(35126, shape=(), dtype=int64)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PrefetchDataset' object has no attribute 'get_shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8601/4044468137.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'PrefetchDataset' object has no attribute 'get_shape'"
     ]
    }
   ],
   "source": [
    "print(dataset.__len__())\n",
    "print(dataset.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(35126, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: {image: (None, None, 3), label: (), name: ()}, types: {image: tf.uint8, label: tf.int64, name: tf.string}>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def asd(im):\n",
    "  count = 0\n",
    "  print(im['image'].get_shape())\n",
    "  #for key, value in im.items() :\n",
    "  #  count = count + 1\n",
    "  #  if (count > 0):\n",
    "  #    print (key, value)\n",
    "  return im\n",
    "\n",
    "dataset.map(asd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(540, 540, 3)\n",
      "(462, 540, 3)\n",
      "(474, 540, 3)\n",
      "(540, 540, 3)\n",
      "(464, 540, 3)\n"
     ]
    }
   ],
   "source": [
    "for image in dataset.take(5):\n",
    "  count = 0\n",
    "  print(image['image'].get_shape())\n",
    "  #for key, value in image.items() :\n",
    "  #  count = count + 1\n",
    "  #  if (count > 0):\n",
    "  #    print (key, value)\n",
    "  count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    /tmp/ipykernel_164238/3998610235.py:6 None  *\n        lambda image: tf.image.resize_with_crop_or_pad(image, 64, 64)\n    /home/juan/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206 wrapper  **\n        return target(*args, **kwargs)\n    /home/juan/.local/lib/python3.8/site-packages/tensorflow/python/ops/image_ops_impl.py:1272 resize_image_with_crop_or_pad\n        image = ops.convert_to_tensor(image, name='image')\n    /home/juan/.local/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py:163 wrapped\n        return func(*args, **kwargs)\n    /home/juan/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1566 convert_to_tensor\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n    /home/juan/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:346 _constant_tensor_conversion_function\n        return constant(v, dtype=dtype, name=name)\n    /home/juan/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:271 constant\n        return _constant_impl(value, dtype, shape, name, verify_shape=False,\n    /home/juan/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:288 _constant_impl\n        tensor_util.make_tensor_proto(\n    /home/juan/.local/lib/python3.8/site-packages/tensorflow/python/framework/tensor_util.py:457 make_tensor_proto\n        _AssertCompatible(values, dtype)\n    /home/juan/.local/lib/python3.8/site-packages/tensorflow/python/framework/tensor_util.py:334 _AssertCompatible\n        raise TypeError(\"Expected any non-tensor type, got a tensor instead.\")\n\n    TypeError: Expected any non-tensor type, got a tensor instead.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_164238/3998610235.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#  input_reSizer(image_batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_with_crop_or_pad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[1;32m   1859\u001b[0m         warnings.warn(\"The `deterministic` argument has no effect unless the \"\n\u001b[1;32m   1860\u001b[0m                       \"`num_parallel_calls` argument is specified.\")\n\u001b[0;32m-> 1861\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1862\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m       return ParallelMapDataset(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   4979\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4980\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preserve_cardinality\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4981\u001b[0;31m     self._map_func = StructuredFunctionWrapper(\n\u001b[0m\u001b[1;32m   4982\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4983\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   4216\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4218\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4219\u001b[0m     \u001b[0;31m# There is no graph to add in eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4220\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3148\u001b[0m          \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorSpec\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3149\u001b[0m     \"\"\"\n\u001b[0;32m-> 3150\u001b[0;31m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0m\u001b[1;32m   3151\u001b[0m         *args, **kwargs)\n\u001b[1;32m   3152\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3114\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3115\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3116\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3117\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3118\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3296\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3297\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3298\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3299\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3300\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   4193\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   4194\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4195\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4196\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4197\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   4123\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_should_unpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4124\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4125\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4126\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_should_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4127\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    693\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    696\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    /tmp/ipykernel_164238/3998610235.py:6 None  *\n        lambda image: tf.image.resize_with_crop_or_pad(image, 64, 64)\n    /home/juan/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206 wrapper  **\n        return target(*args, **kwargs)\n    /home/juan/.local/lib/python3.8/site-packages/tensorflow/python/ops/image_ops_impl.py:1272 resize_image_with_crop_or_pad\n        image = ops.convert_to_tensor(image, name='image')\n    /home/juan/.local/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py:163 wrapped\n        return func(*args, **kwargs)\n    /home/juan/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1566 convert_to_tensor\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n    /home/juan/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:346 _constant_tensor_conversion_function\n        return constant(v, dtype=dtype, name=name)\n    /home/juan/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:271 constant\n        return _constant_impl(value, dtype, shape, name, verify_shape=False,\n    /home/juan/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:288 _constant_impl\n        tensor_util.make_tensor_proto(\n    /home/juan/.local/lib/python3.8/site-packages/tensorflow/python/framework/tensor_util.py:457 make_tensor_proto\n        _AssertCompatible(values, dtype)\n    /home/juan/.local/lib/python3.8/site-packages/tensorflow/python/framework/tensor_util.py:334 _AssertCompatible\n        raise TypeError(\"Expected any non-tensor type, got a tensor instead.\")\n\n    TypeError: Expected any non-tensor type, got a tensor instead.\n"
     ]
    }
   ],
   "source": [
    "#input_reSizer = tf.keras.Sequential()\n",
    "#input_reSizer.add(layers.Reshape((3,64,64)))\n",
    "#for image_batch in dataset:\n",
    "#  input_reSizer(image_batch\n",
    "dataset = dataset.map(lambda image: tf.image.resize_with_crop_or_pad(image, 64, 64))\n",
    "#train_dataset = dataset.batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='diabetic_retinopathy_detection',\n",
       "    full_name='diabetic_retinopathy_detection/btgraham-300/3.0.0',\n",
       "    description=\"\"\"\n",
       "    A large set of high-resolution retina images taken under a variety of imaging conditions.\n",
       "    \"\"\",\n",
       "    config_description=\"\"\"\n",
       "    Images have been preprocessed as the winner of the Kaggle competition did in 2015: first they are resized so that the radius of an eyeball is 300 pixels, then they are cropped to 90% of the radius, and finally they are encoded with 72 JPEG quality.\n",
       "    \"\"\",\n",
       "    homepage='https://www.kaggle.com/c/diabetic-retinopathy-detection/data',\n",
       "    data_path='/home/juan/tensorflow_datasets/diabetic_retinopathy_detection/btgraham-300/3.0.0',\n",
       "    download_size=1.13 MiB,\n",
       "    dataset_size=3.64 GiB,\n",
       "    features=FeaturesDict({\n",
       "        'image': Image(shape=(None, None, 3), dtype=tf.uint8),\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=5),\n",
       "        'name': Text(shape=(), dtype=tf.string),\n",
       "    }),\n",
       "    supervised_keys=None,\n",
       "    disable_shuffling=False,\n",
       "    splits={\n",
       "        'sample': <SplitInfo num_examples=10, num_shards=1>,\n",
       "        'test': <SplitInfo num_examples=42670, num_shards=16>,\n",
       "        'train': <SplitInfo num_examples=35126, num_shards=16>,\n",
       "        'validation': <SplitInfo num_examples=10906, num_shards=4>,\n",
       "    },\n",
       "    citation=\"\"\"@ONLINE {kaggle-diabetic-retinopathy,\n",
       "        author = \"Kaggle and EyePacs\",\n",
       "        title  = \"Kaggle Diabetic Retinopathy Detection\",\n",
       "        month  = \"jul\",\n",
       "        year   = \"2015\",\n",
       "        url    = \"https://www.kaggle.com/c/diabetic-retinopathy-detection/data\"\n",
       "    }\"\"\",\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "  aKernelSize = 5\n",
    "  aStrides = 2\n",
    "  z_dim = 100\n",
    "\n",
    "  layersInfo = [(4, 1024), (8, 512), (16,256), (32, 128), (64, 3)]\n",
    "  \n",
    "  layer_0 = (4, 1024)\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(layers.Dense(layer_0[0] ** 2 * layer_0[1], use_bias=False, input_shape=(z_dim,)))\n",
    "  model.add(layers.BatchNormalization())\n",
    "  model.add(layers.LeakyReLU())\n",
    "\n",
    "  \n",
    "  model.add(layers.Reshape((layer_0[0], layer_0[0], layer_0[1])))\n",
    "  print(model.output_shape)\n",
    "  assert model.output_shape == (None, layer_0[0], layer_0[0], layer_0[1])  # Note: None is the batch size\n",
    "\n",
    "  layer_1 = layersInfo[1]\n",
    "  model.add(layers.Conv2DTranspose(layer_1[1], kernel_size=aKernelSize, strides=aStrides, padding='same', use_bias=False))\n",
    "  print(model.output_shape)\n",
    "  assert model.output_shape == (None, layer_1[0], layer_1[0], layer_1[1])\n",
    "  model.add(layers.BatchNormalization())\n",
    "  model.add(layers.LeakyReLU())\n",
    "\n",
    "  layer_2 = layersInfo[2]\n",
    "  model.add(layers.Conv2DTranspose(layer_2[1], kernel_size=aKernelSize, strides=aStrides, padding='same', use_bias=False))\n",
    "  assert model.output_shape == (None, layer_2[0], layer_2[0], layer_2[1])\n",
    "  model.add(layers.BatchNormalization())\n",
    "  model.add(layers.LeakyReLU())\n",
    "\n",
    "  layer_3 = layersInfo[3]\n",
    "  model.add(layers.Conv2DTranspose(layer_3[1], kernel_size=aKernelSize, strides=aStrides, padding='same', use_bias=False))\n",
    "  assert model.output_shape == (None, layer_3[0], layer_3[0], layer_3[1])\n",
    "  model.add(layers.BatchNormalization())\n",
    "  model.add(layers.LeakyReLU())\n",
    "\n",
    "  layer_4 = layersInfo[4]\n",
    "  model.add(layers.Conv2DTranspose(layer_4[1], kernel_size=aKernelSize, strides=aStrides, padding='same', use_bias=False, activation='tanh'))\n",
    "  assert model.output_shape == (None, layer_4[0], layer_4[0], layer_4[1])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 4, 4, 1024)\n",
      "(None, 8, 8, 512)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f04721ee6d0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = make_generator_model()\n",
    "\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 4, 4, 1024)\n",
      "(None, 8, 8, 512)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7f047223b6a0>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_generator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "  aKernelSize = 5\n",
    "  aStrides = 2\n",
    "  dropOutRatio = 0.3\n",
    "\n",
    "  layersInfo = [(32, 64), (16,128), (8, 256), (4, 512)]\n",
    "  \n",
    "  model = tf.keras.Sequential()\n",
    "  \n",
    "  layer_0 = layersInfo[0]\n",
    "  model.add(layers.Conv2D(layer_0[1], kernel_size=aKernelSize, strides=aStrides, padding='same', input_shape=[64, 64, 3]))\n",
    "  model.add(layers.LeakyReLU())\n",
    "  model.add(layers.Dropout(dropOutRatio))\n",
    "\n",
    "  layer_1 = layersInfo[1]\n",
    "  model.add(layers.Conv2D(layer_1[1], kernel_size=aKernelSize, strides=aStrides, padding='same'))\n",
    "  model.add(layers.LeakyReLU())\n",
    "  model.add(layers.Dropout(dropOutRatio))\n",
    "\n",
    "  layer_2 = layersInfo[2]\n",
    "  model.add(layers.Conv2D(layer_2[1], kernel_size=aKernelSize, strides=aStrides, padding='same'))\n",
    "  model.add(layers.LeakyReLU())\n",
    "  model.add(layers.Dropout(dropOutRatio))\n",
    "\n",
    "  layer_3 = layersInfo[3]\n",
    "  model.add(layers.Conv2D(layer_3[1], kernel_size=aKernelSize, strides=aStrides, padding='same'))\n",
    "  model.add(layers.LeakyReLU())\n",
    "  model.add(layers.Dropout(dropOutRatio))\n",
    "\n",
    "  model.add(layers.Flatten())\n",
    "  model.add(layers.Dense(1))\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.0001704]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image)\n",
    "print (decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<keras.layers.convolutional.Conv2D object at 0x7f0472144250>, <keras.layers.advanced_activations.LeakyReLU object at 0x7f047214bbe0>, <keras.layers.core.Dropout object at 0x7f0472155ca0>, <keras.layers.convolutional.Conv2D object at 0x7f047216ab20>, <keras.layers.advanced_activations.LeakyReLU object at 0x7f047216a850>, <keras.layers.core.Dropout object at 0x7f0472155bb0>, <keras.layers.convolutional.Conv2D object at 0x7f047215c550>, <keras.layers.advanced_activations.LeakyReLU object at 0x7f047215cd60>, <keras.layers.core.Dropout object at 0x7f047216a670>, <keras.layers.convolutional.Conv2D object at 0x7f0472160a90>, <keras.layers.advanced_activations.LeakyReLU object at 0x7f0472157bb0>, <keras.layers.core.Dropout object at 0x7f0472170be0>, <keras.layers.core.Flatten object at 0x7f0472170c40>, <keras.layers.core.Dense object at 0x7f04721606d0>]\n"
     ]
    }
   ],
   "source": [
    "print(discriminator.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learing_rate = 1e-4\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learing_rate)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learing_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# You will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_images = generator(noise, training=True)\n",
    "\n",
    "      real_output = discriminator(images, training=True)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(epoch_no):\n",
    "  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "  predictions = model(test_input, training=False)\n",
    "\n",
    "  fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "      plt.axis('off')\n",
    "\n",
    "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for image_batch in dataset:\n",
    "      train_step(image_batch)\n",
    "\n",
    "    # Save the model every 15 epochs\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "  # Generate after the final epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-07 22:27:51.907293: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Cannot batch tensors with different shapes in component 0. First element had shape [540,540,3] and element 1 had shape [462,540,3]. [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_164238/2228458018.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_164238/2044935595.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimage_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m       \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    745\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2726\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2727\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2728\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2729\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2730\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6939\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6940\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6941\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6942\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot batch tensors with different shapes in component 0. First element had shape [540,540,3] and element 1 had shape [462,540,3]. [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "train(train_dataset, EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
